/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41598'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:46131'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:37110'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:44005'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:45204'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:37697'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:45818'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:44786'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41086'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:45990'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:43121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41951'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41677'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:42786'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:44391'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:35377'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41066'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:45065'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:36502'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:44336'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41121'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:45205'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:43936'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:41213'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:39827'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:38621'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:43076'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.184:38821'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:35666
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:44056
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:43956
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:37123
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:38542
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:34579
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:35666
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:44056
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:41623
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:43956
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:37123
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:38542
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:34579
distributed.worker - INFO -          dashboard at:         172.30.6.184:43118
distributed.worker - INFO -          dashboard at:         172.30.6.184:42937
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:41623
distributed.worker - INFO -          dashboard at:         172.30.6.184:40034
distributed.worker - INFO -          dashboard at:         172.30.6.184:44937
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:40599
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:38002
distributed.worker - INFO -          dashboard at:         172.30.6.184:44161
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.184:43797
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:38003
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.184:34684
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:40599
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:32922
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:38002
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:42625
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:32983
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:38003
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:34931
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:45004
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:36469
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.184:42060
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:42658
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:42691
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:32922
distributed.worker - INFO -          dashboard at:         172.30.6.184:33953
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:40027
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:42625
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.6.184:37338
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:32983
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:43795
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:34155
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:34931
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:36469
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:45004
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:42658
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:42691
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.184:37495
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:46182
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:43601
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:38893
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:40027
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:36413
distributed.worker - INFO -          dashboard at:         172.30.6.184:35361
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.6.184:38236
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:45172
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:38929
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:43795
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:34155
distributed.worker - INFO -          dashboard at:         172.30.6.184:43437
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.184:34688
distributed.worker - INFO -          dashboard at:         172.30.6.184:46514
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.184:33909
distributed.worker - INFO -          dashboard at:         172.30.6.184:39742
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3y8d4top
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:46182
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-b20mw419
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:38893
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:43601
distributed.worker - INFO -          dashboard at:         172.30.6.184:44309
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-s8l7fbza
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-j8rco99r
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:36413
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:45172
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:38929
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.6.184:36082
distributed.worker - INFO -          dashboard at:         172.30.6.184:44115
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3b3ss93s
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-uohae8va
distributed.worker - INFO -          dashboard at:         172.30.6.184:37938
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.6.184:41798
distributed.worker - INFO -          dashboard at:         172.30.6.184:39125
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-3t95bt85
distributed.worker - INFO -          dashboard at:         172.30.6.184:43932
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.184:33131
distributed.worker - INFO -          dashboard at:         172.30.6.184:33753
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-a7vpz9b3
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-5k6lwjq0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-t93z9uyw
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ddjxhiil
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hya6r645
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r5filxjr
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kas78ic7
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dsetvekr
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2rszsrmh
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wx57ve74
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-76pa34l5
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hfz9e6gi
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yod9tqbp
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-r2x2dx9q
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-kqy74ucw
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qphtvqls
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ae_z56g_
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-6tj2a0l4
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sttb_8dx
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-23s4wi2u
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:41857
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:41857
distributed.worker - INFO -          dashboard at:         172.30.6.184:42680
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2_t4z0wg
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 10.06 MB from 2559 reference cycles (threshold: 10.00 MB)
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.6.184:38003
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1841, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 3039, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.6.184:38003
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1841, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 3039, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-e72a1093d919f731c1048492fb6db6d8', 43, 1, 1)
distributed.nanny - WARNING - Worker process 33129 was killed by signal 15
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-da5f5ff3c6a2e0cd35dce872b5b17cdc', 81, 0, 1)
distributed.worker - INFO - Dependent not found: ('rechunk-split-e72a1093d919f731c1048492fb6db6d8', 1823) 0 .  Asking scheduler
distributed.worker - INFO - Dependent not found: ('rechunk-split-da5f5ff3c6a2e0cd35dce872b5b17cdc', 3406) 0 .  Asking scheduler
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:46668
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:46668
distributed.worker - INFO -          dashboard at:         172.30.6.184:34373
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-rkkhlf93
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.nanny - WARNING - Worker exceeded 95% memory budget. Restarting
distributed.nanny - WARNING - Worker process 33115 was killed by signal 15
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
distributed.nanny - WARNING - Restarting worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.184:36013
distributed.worker - INFO -          Listening to:   tcp://172.30.6.184:36013
distributed.worker - INFO -          dashboard at:         172.30.6.184:42114
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-sex0sugq
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:38893
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:46668
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:44786'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:32983
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:37123
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:41623
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:40599
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:34155
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:45172
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:34579
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:38002
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:43601
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41086'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:35666
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:32922
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:36413
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:42625
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:43956
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:36469
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:42658
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:42691
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:36013
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:46182
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:45004
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:40027
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:38929
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:38542
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:34931
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:44056
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.184:43795
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:45990'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:37110'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:43076'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41213'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:43936'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:35377'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:44336'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:44391'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:45205'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:45818'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41066'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41951'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:37697'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41598'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:36502'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:46131'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:42786'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:44005'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:43121'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41121'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:39827'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:41677'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:38821'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:45065'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:38621'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.184:45204'
distributed.dask_worker - INFO - End worker
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
