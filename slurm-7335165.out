/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:34704'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:44451'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:34393'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:42332'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:42740'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:34500'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35887'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35548'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:39713'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:40971'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:45897'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:38749'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35058'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:33133'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35963'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35908'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:40374'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:38058'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:44346'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:37624'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:34407'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:38204'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:37073'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:44737'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:34800'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:35156'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:46863'
distributed.nanny - INFO -         Start Nanny at: 'tcp://172.30.6.178:42417'
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:46409
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:33923
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:44949
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:46847
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:46409
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:39143
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:33923
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:44949
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:46847
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:44560
distributed.worker - INFO -          dashboard at:         172.30.6.178:42115
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:34348
distributed.worker - INFO -          dashboard at:         172.30.6.178:36368
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:39143
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:36582
distributed.worker - INFO -          dashboard at:         172.30.6.178:41644
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:35701
distributed.worker - INFO -          dashboard at:         172.30.6.178:44287
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:39988
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:44560
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:34348
distributed.worker - INFO -          dashboard at:         172.30.6.178:39904
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:43161
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:36582
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:35701
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:39988
distributed.worker - INFO -          dashboard at:         172.30.6.178:36066
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.178:43766
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:43161
distributed.worker - INFO -          dashboard at:         172.30.6.178:43272
distributed.worker - INFO -          dashboard at:         172.30.6.178:43678
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:44296
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.178:35111
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:42092
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:43190
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          dashboard at:         172.30.6.178:37730
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:42380
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:42317
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:38281
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:44296
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:42092
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:43190
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:33057
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:40311
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:39665
distributed.worker - INFO -                Memory:                    4.29 GB
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:45365
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:35060
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:36474
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:42380
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-p_jt1jts
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:42317
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:38281
distributed.worker - INFO -          dashboard at:         172.30.6.178:37629
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-wd77f5jm
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:35110
distributed.worker - INFO -          dashboard at:         172.30.6.178:37996
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:45037
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.178:43030
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:42621
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:35110
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hnxsb67y
distributed.worker - INFO -               Threads:                          1
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:43361
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:33057
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:40311
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-8kqtj2_y
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:39665
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:45365
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:35060
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:36474
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          dashboard at:         172.30.6.178:38585
distributed.worker - INFO -          dashboard at:         172.30.6.178:41541
distributed.worker - INFO -          dashboard at:         172.30.6.178:37719
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:45037
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-46_6s_yv
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:42621
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:         172.30.6.178:35877
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:43361
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -          dashboard at:         172.30.6.178:40039
distributed.worker - INFO -          dashboard at:         172.30.6.178:44171
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7srd3f0j
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-yf73v9ow
distributed.worker - INFO -          dashboard at:         172.30.6.178:40232
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.178:37322
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.178:33809
distributed.worker - INFO -          dashboard at:         172.30.6.178:42687
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -          dashboard at:         172.30.6.178:36737
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-ljh8wjhm
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-q1_ycyqe
distributed.worker - INFO -          dashboard at:         172.30.6.178:41767
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2hn8q18k
distributed.worker - INFO -          dashboard at:         172.30.6.178:39586
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-qgjr1la4
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-7cjzwzie
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-hg8r832w
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-phmwtye8
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-zuvc0c4t
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0d2los52
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-9a3fti09
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-0_o6g_qc
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-2iykq5yy
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oqlyapqz
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-w6et2quz
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-dh9lyctk
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-bj4oghsv
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-g_okio_c
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-oabsnumc
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-pil7gqnp
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-njtlueo0
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/contextlib.py:119: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - INFO -       Start worker at:   tcp://172.30.6.178:37466
distributed.worker - INFO -          Listening to:   tcp://172.30.6.178:37466
distributed.worker - INFO -          dashboard at:         172.30.6.178:43721
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -               Threads:                          1
distributed.worker - INFO -                Memory:                    4.29 GB
distributed.worker - INFO -       Local Directory: /scratch/cnt0024/hmg2840/albert7a/daskjobqueue/worker-doa6553m
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO -         Registered to:   tcp://172.30.100.2:38861
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.worker - INFO - -------------------------------------------------
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.core - INFO - Starting established connection
distributed.utils_perf - INFO - full garbage collection released 13.19 MB from 76 reference cycles (threshold: 10.00 MB)
distributed.core - INFO - Event loop was unresponsive in Worker for 3.21s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - ERROR - Worker stream died during communication: tcp://172.30.6.184:38003
Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 194, in read
    n_frames = yield stream.read_bytes(8)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 1841, in gather_dep
    self.rpc, deps, worker, who=self.address
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/worker.py", line 3039, in get_data_from_worker
    max_connections=max_connections,
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/core.py", line 535, in send_recv
    response = yield comm.read(deserializers=deserializers)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 729, in run
    value = future.result()
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/tornado/gen.py", line 736, in run
    yielded = self.gen.throw(*exc_info)  # type: ignore
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 214, in read
    convert_stream_closed_error(self, e)
  File "/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/comm/tcp.py", line 139, in convert_stream_closed_error
    raise CommClosedError("in %s: %s: %s" % (obj, exc.__class__.__name__, exc))
distributed.comm.core.CommClosedError: in <closed TCP>: ConnectionResetError: [Errno 104] Connection reset by peer
distributed.worker - INFO - Can't find dependencies for key ('rechunk-merge-da5f5ff3c6a2e0cd35dce872b5b17cdc', 81, 0, 0)
distributed.worker - INFO - Dependent not found: ('rechunk-split-da5f5ff3c6a2e0cd35dce872b5b17cdc', 3403) 0 .  Asking scheduler
distributed.core - INFO - Event loop was unresponsive in Worker for 4.96s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 5.00s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.80s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.67s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 4.65s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 8.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.core - INFO - Event loop was unresponsive in Worker for 35.33s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - -------------------------------------------------
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - WARNING - Heartbeat to scheduler failed
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - INFO - Waiting to connect to:   tcp://172.30.100.2:38861
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:33057
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:43190
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:44560
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:39713'
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:35060
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:42317
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:35701
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:40311
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:46847
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:39143
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:36582
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:43361
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:42740'
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:42621
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:39665
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:35110
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:44949
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:43161
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:42092
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:46409
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:37466
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:34348
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:36474
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:44296
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:45037
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:38281
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:45365
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:33923
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:42380
distributed.worker - ERROR - Timed out when connecting to scheduler 'tcp://172.30.100.2:38861'
NoneType: None
distributed.worker - INFO - Stopping worker at tcp://172.30.6.178:39988
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:38204'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:35156'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:35887'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:35963'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:38058'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:35908'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:37624'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:35058'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:44346'
distributed.nanny - INFO - Closing Nanny at 'tcp://172.30.6.178:33133'
distributed.dask_worker - INFO - End worker
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
distributed.process - WARNING - reaping stray process <ForkServerProcess(Dask Worker process (from Nanny), started daemon)>
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 1 leaked semaphores to clean up at shutdown
  len(cache))
/scratch/cnt0024/hmg2840/albert7a/anaconda3/lib/python3.7/site-packages/distributed/utils.py:136: RuntimeWarning: Couldn't detect a suitable IP address for reaching '8.8.8.8', defaulting to '127.0.0.1': [Errno 101] Network is unreachable
  RuntimeWarning,
